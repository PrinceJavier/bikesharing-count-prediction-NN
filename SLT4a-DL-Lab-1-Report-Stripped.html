<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <title>A4</title>

  <!-- Normalize or reset CSS with your favorite library -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/7.0.0/normalize.min.css">

  <!-- Load paper.css for happy printing -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/paper-css/0.4.1/paper.css">
  <link rel="stylesheet" href="custom.css">

  <!-- Set page size here: A5, A4 or A3 -->
  <!-- Set also "landscape" if you need -->
  <style>@page { size: A4 }</style>

    <!-- Loading mathjax macro -->
    <!-- Load mathjax -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML"></script>
    <!-- MathJax configuration -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: true,
            processEnvironments: true
        },
        // Center justify equations in code and markdown cells. Elsewhere
        // we use CSS to left justify single line equations in code cells.
        displayAlign: 'center',
        "HTML-CSS": {
            styles: {'.MathJax_Display': {"margin": 0}},
            linebreaks: { automatic: true }
        }
    });
    </script>
    <!-- End of mathjax configuration -->


</head>

<!-- Set "A5", "A4" or "A3" for class name -->
<!-- Set also "landscape" if you need -->
<body class="A4">

  <!-- Each sheet element should have the class "sheet" -->
  <!-- "padding-**mm" is optional: you can set 10, 15, 20 or 25 -->
  <section class="sheet padding-20mm">

    <!-- Write HTML just like a web page -->
    <div id="rtgd-report-title">
            Analyzing Bike Sharing Systems: A Comparison of Machine Learning Models
    </div>
    <p id="rtgd-report-author">Prince Joseph Erneszer Javier, Reynaldo Tugade Jr.</p>
    <p id="rtgd-report-affiliation">Asian Institute of Management, Makati, Philippines</p>
    <p id="rtgd-report-contact-info">pjavier@aim.edu, rtugade@aim.edu</p>

    <div id="rtgd-abstract">
            <div class="rtgd-section-title">Abstract</div>
            In performing data analysis, a common task is to search the most appropriate algorithm(s) to best 
            resemble a given system. In this report, we demonstrate the suitability of using a neural network 
            in predicting the potential number of bicycle-sharing users using combined historical rental 
            and weather information. The idea is to augment previous machine learning models 
            and discover the possibility of getting better test accuracy. We used K-Nearest 
            Neighbor, Linear Regression, Ridge Regression, Lasso Regression, 
            Linear Support Vector Machine, Decision Trees, Random Forest, 
            and Gradient Boosting Method as baseline machine learning models. We used a 3 layer fully-connected 
            feed-forward network with 30 hidden nodes. This report shows that such configuration works 
            well the most with 73.2% $r^2$ and  52.7% MAPE. In comparison, RF could predict with 67.9% 
            $r^2$ and 75.9% MAPE while GBM could predict with 67.2% $r^2$ and 68.8% MAPE.
            </div>
            
            <p class="rtgd-section-title">1 Introduction</p>
            
            <p class="rtgd-section-content">
            Bicycle-sharing systems are paid services where an individual can rent an available bicycle on a short-term basis. Used to be considered as a service only available in small and closed communities (e.g. campuses, private subdivisions), bicycle-sharing systems are becoming mainstream modes for public-transport in several countries. Few of these systems include Paris' "Vellib" which started operating in 2005, Hangzhou's bicycle hub in China which houses more than 50,000 bicycles and even locally with Asian Development Bank's (ADB) Sustainable Transport Initiative program. 
            <br>
            <br>
            &nbsp;&nbsp;&nbsp;&nbsp;From a sustainability perspective, bicycle-sharing systems have its benefits. One, it promotes better flexible mobility. Bicycle stations can be placed anywhere especially in areas where there's a perceived concentration of people traveling. Second, it impacts emission reduction due to no fuel use and reduces congestion. Third, it's relatively cheap and very convenient specifically since it helps improve multimodal transport connections.
            <br>
            <br>
            &nbsp;&nbsp;&nbsp;&nbsp;In this regard, we wish to explore interesting behavior found in bicycle-sharing systems. The richness of data involved in bicycle-sharing systems can provide more information from a mobility sensing perspective. In this report, we will explore potential users based on previous rentals and weather data. We will leverage on learned Machine Learning and Neural network techniques to contrast and compare which among these models best resemble the bicycle-sharing system.
            </p>
          
            <p class="rtgd-section-title">2 Data</p>
            <p class="rtgd-section-content">
            The original dataset comes from the Capital Bikeshare website which is compiled by the Laboratory of Artificial Intelligence and Decision Support (LIAAD), University of Porto and placed in UCL. The dataset contains time-series rental and weather information.
            </p>
            
        </section>
        <!-- Each sheet element should have the class "sheet" -->
        <!-- "padding-**mm" is optional: you can set 10, 15, 20 or 25 -->
        <section class="sheet padding-20mm">

            <p class="rtgd-table-caption">Table 1: Available features found in the dataset.</p>
            <table id="rtgd-table">
                    <tr>
                        <th><b>Feature Variable</b></th>	
                        <th><b>Possible Values</b></th>	
                        <th><b>Description</b></th>
                    </tr><tr>
                    <td>instant</td>	
                    <td>0 to 17378</td>	
                    <td>Index of the record</td>
                    </tr><tr>
                    <td>dteday</td>	
                    <td>2011-01-01 to 2012-12-31</td>
                    <td>Date</td>
                    </tr><tr>
                    <td>season</td>
                    <td>1,2,3,4</td>	
                    <td>Season (Spring, Summer, Fall, Winter)</td>
                    </tr><tr>
                    <td>yr</td>
                    <td>0,1</td>	
                    <td>Year of occurrence</td>
                    </tr><tr>
                    <td>mnth</td>	
                    <td>1,2,3,...,12</td>	
                    <td>Month</td>
                    </tr>
                    <tr>
                    <td>hr</td>	
                    <td>0,1,2,...,23</td>	
                    <td>Hour</td>
                    </tr><tr>
                    <td>holiday</td>	
                    <td>0,1</td>	
                    <td>Whether current day is a holiday. Based from (dc.gov)[Holidays]</td>
                    </tr><tr>
                    <td>weekday</td>
                    <td>0,1,2,...,6</td>	
                    <td>Day of the week</td>
                    </tr><tr>
                    <td>weathersit</td>	
                    <td>1,2,3,4</td>	
                    <td>Weather information based meteorological events</td>
                    </tr><tr>
                    <td>_</td>	
                    <td>_</td>	
                    <td>(1) Clear, Few clouds, Partly cloudy</td>
                    </tr><tr>
                    <td>_</td>
                    <td>_</td>	
                    <td>(2) Misty plus still generally cloudy environment</td>
                    </tr><tr>
                    <td>_</td>
                    <td>_</td>	
                    <td>(3) Light Snow, Light Rain with occasional Thunderstorms,Light Rain with scattered clouds</td>
                    </tr><tr>
                    <td>_</td>
                    <td>_</td>
                    <td>(4) Heavy Rain with Ice pellets, Thunderstorm with Mist, Snow with Fog</td>
                    </tr><tr>
                    <td>temp</td>
                    <td>0.02 to 1.00</td>
                    <td>Normalized feeling temperature in Celsius.</td>
                    </tr><tr>
                    <td>atemp</td>	
                    <td>0.0000 to 1.0000</td>	
                    <td>Normalized feeling temperature in Celsius. </td>
                    </tr><tr>
                    <td>humz</td>	
                    <td>0.00 to 1.00</td>	
                    <td>Normalized humidity</td>
                    </tr><tr>
                    <td>windspeed</td>	
                    <td>0.0000 to 0.8507</td>	
                    <td>Normalized windspeed</td>
                    </tr><tr>
                    <td>casual</td>	
                    <td>0 to 367</td>	
                    <td>Count of casual users</td>
                    </tr><tr>
                    <td>registered</td>	
                    <td>0 to 886</td>	
                    <td>Count of registered users</td>
                    </tr><tr>
                    <td>cnt</td>
                    <td>1 to 977</td>	
                    <td>Count of total rental bikes including both casual and registered</td>
                    </tr>
             </table>
            <br>
            <p class="rtgd-section-content">
            * Temperature variable <i>(temp)</i> is computed using the following equation: 
            \begin{equation}
            \frac{t - t_{min}}{t_{max} - t_{min}}, t_{min}= -8, t_{max}= +39 \tag{1}
            \end{equation}
                * Absolute temperature variable <i>(atemp)</i>: 
            \begin{equation}
            \frac{t - t_{min}}{t_{max} - t_{min}}, t_{min}= -16, t_{max}=+50 \tag{2}
            \end{equation}
            </p>

            <p class="rtgd-section-title">3 Methodology</p>

            <p class="rtgd-section-subtitle">3.1 Data Preprocessing</p>
            <p class="rtgd-section-content">
            Features selected were year, holiday, temp, hum, windspeed, season, weathersit, mnth, hr, and weekday. The target variable was cnt. One-hot encoding was applied on season, weathersit, mnth, hr, and weekday. A bias was added as an additional column having a single value of 1.00. The resulting features data including bias contained 56 features. The features were then scaled using min-max scaling given by:
            <br><br>
            \begin{equation}
            X_{scaled} = \frac{X - min(X)}{max(X) - min(X)} \tag{3}
            \end{equation}
            <br>
            where X is the feature matrix. Since the maximum value of cnt was found to be 977, cnt was divided by 1000 to scale it to values between 0 and 1. The last 20 days were set for testing set. In the remaining dataset, the last 60 days were set as the validation set. The remaining dataset was used to train the machine learning models. The validation set was used to evaluate the model during training while the testing set was used to test the accuracy of the model after training using the best parameters.
            </p>

        </section>
        <!-- Each sheet element should have the class "sheet" -->
        <!-- "padding-**mm" is optional: you can set 10, 15, 20 or 25 -->
        <section class="sheet padding-20mm">

            <p class="rtgd-section-subtitle">3.2 Neural Network Modeling</p>
            <p class="rtgd-section-content">
            A feed-forward neural network having 56 input nodes, 30 nodes in one hidden layer, and 1 output node was developed. The learning rate from input to hidden and hidden to output were both 0.001. The input, hidden, and output activation functions were linear, sigmoid, and sigmoid respectively. These parameters were selected through a genetic algorithm optimizer that aimed to maximize validation accuracy.
            </p>
            <p class="rtgd-table-caption">Table 2: The ranges of parameter values fed into the genetic algorithm.</p>
            <table id="rtgd-table">
                    <tr>
                        <th><b>Parameter</b></th>	
                        <th><b>Range of values</b></th>	
                    </tr>
                    <tr>
                        <td>hidden nodes</td>	
                        <td>20, 22, 24,...,54, 56</td>	
                    </tr>
                    <tr>
                        <td>learning rates</td>	
                        <td>0.001, 0.0001</td>
                    </tr>
                    <tr>
                        <td>activation functions</td>	
                        <td>sin, relu, tanh, sigmoid</td>
                    </tr>
            </table> 
            <br>

            <p class="rtgd-section-content">
            The loss function being minimized by the neural network is given by:
            <br><br>
            \begin{equation}
            Error = \frac{1}{2}(\Psi_{NN} - \Psi_{true})^2 \tag{4}
            \end{equation}
            <br>
            where $\Psi_{NN}$ is the predicted value and $\Psi_{true}$ is the true value. The neural network was trained and validated using the training and validation sets over 5,000 iterations. The testing set was used to evaluate the predictive accuracy of the model using the coefficient of determination ($r^2$) and mean absolute percentage error (MAPE). The equations are shown below.
                    
            \begin{equation}
            r^2 = \frac{\sum_i(\hat y_i-\bar y)}{\sum_i(y_i-\bar y)} \tag{5}
            \end{equation}
            <br>   
            \begin{equation}
            MAPE = \frac{100\%}{n}\sum_i{\left | \frac{y_i - \hat y_i}{y_i}\right |} \tag{6}
            \end{equation}
               
            where $\hat y_i$ is the predicted value, $y_i$ is the true value, $\bar y$ is the mean of true values, and $n$ is the number of samples.
            </p>
            
            <p class="rtgd-section-subtitle">3.2 Machine Learning Modeling</p>

            <p class="rtgd-section-content">
            Eight more regression models were trained on the same training dataset, namely k nearest neighbors (kNN), linear regression, lasso regression, ridge regression, linear support vector machines (LSVM), decision tree, random forest, and gradient boosting machines (GBM).

            Using the optimal parameter, the target values were predicted using the test set. The accuracy was measured as the $r^2$ and MAPE between the true values and predicted values.
            </p>
            <p class="rtgd-table-caption">Table 3: Summary of hyperparameters tweaked for each model.</p>
            <table id="rtgd-table">
                    <tr>
                        <th>Model</th>	
                        <th>Parameters</th>	
                    </tr>
                    <tr>
                        <td>Feed-forward NN</td>	
                        <td>no. of nodes<br>no. of hidden layers<br>activation functions<br>learning rates</td>	
                    </tr>
                    <tr>
                        <td>k-Nearest Neighbor</td>	
                        <td>no. of nearest neighbors</td>	
                    </tr>
                    <tr>
                        <td>Linear Regression</td>	
                        <td>-</td>	
                    </tr>
                    <tr>
                        <td>Lasso Regression</td>	
                        <td>alpha</td>	
                    </tr>
                    <tr>
                        <td>Ridge Regression</td>	
                        <td>alpha</td>	
                    </tr>
                    <tr>
                        <td>Linear Support Vector Machine</td>	
                        <td>C</td>	
                    </tr>
                    <tr>
                        <td>Decision Tree</td>	
                        <td>max-depth</td>	
                    </tr>
                    <tr>
                        <td>Random Forest</td>	
                        <td>max-depth</td>	
                    </tr>
                    <tr>
                        <td>Gradient Boosting</td>	
                        <td>max-depth</td>	
                    </tr>
            </table>
            <br>
            <p class="rtgd-section-content">
            Each model was trained and validated on a range of parameters. The parameters that gave the highest $r^2$ on the validation set were identified as the optimal parameters.
            </p>
            
        </section>
        <!-- Each sheet element should have the class "sheet" -->
        <!-- "padding-**mm" is optional: you can set 10, 15, 20 or 25 -->
        <section class="sheet padding-20mm">


            <p class="rtgd-section-title">4 Results</p>

            <p class="rtgd-section-content">
            The feed forward neural network was able to predict bike-sharing counts on the test set with 73.2% accuracy, higher than the eight other machine learning models. The MAPE obtained was the lowest at 52.7%.
            </p>

        

        <p class="rtgd-table-caption">Table 4: Summary of the predictive accuracies and corresponding parameters of all models evaluated.</p>
            <table id="rtgd-table">
                    <tr>
                        <th>Model</th>	
                        <th>Parameters</th>
                        <th>Values</th>	
                        <th>$r^2$ Test Accuracy</th>	
                        <th>Test MAPE</th>
                    </tr>
                    <tr>
                        <td>Feed-forward NN</td>
                        <td>no. of hidden nodes<br>no. of hidden layers<br>activation functions<br>learning rates</td>
                        <td>30 <br>1 <br>(linear, sigmoid, sigmoid) <br>(0.001, 0.001)</td>	
                        <td>73.2%</td>	
                        <td>52.7%</td>	
                    </tr>
                    <tr>
                        <td>Random Forest</td>	
                        <td>max-depth</td>
                        <td>33</td>	
                        <td>67.9%</td>	
                        <td>75.9%</td>	
                    </tr>
                    <tr>
                        <td>Gradient Boosting Method</td>	
                        <td>max-depth</td>
                        <td>22</td>	
                        <td>67.2%</td>	
                        <td>68.5%</td>	
                    </tr>
                    <tr>
                        <td>k-Nearest Neighbor</td>	
                        <td>no. of nearest neighbors</td>
                        <td>2</td>	
                        <td>62.9%</td>	
                        <td>134.9%</td>		
                    </tr>
                    <tr>
                        <td>Lasso Regression</td>	
                        <td>alpha</td>
                        <td>0.0001</td>	
                        <td>48.1%</td>	
                        <td>382.5%</td>		
                    </tr>
                    <tr>
                        <td>Ridge Regression</td>	
                        <td>alpha</td>	
                        <td>10</td>	
                        <td>48.0%</td>	
                        <td>399.4%</td>	
                    </tr>
                    <tr>
                        <td>Linear Regression</td>	
                        <td>-</td>
                        <td>-</td>	
                        <td>47.1%</td>	
                        <td>424.4%</td>		
                    </tr>
                    <tr>
                        <td>Decision Tree</td>	
                        <td>max-depth</td>
                        <td>24</td>	
                        <td>46.0%</td>	
                        <td>75.3%</td>		
                    </tr>    
                    <tr>
                        <td>Linear Support Vector Machine</td>	
                        <td>C</td>
                        <td>1</td>	
                        <td>42.5%</td>	
                        <td>303.6%</td>		
                    </tr>
            </table>

            <p class="rtgd-section-title">5 Conclusion</p>

            <p class="rtgd-section-content">
            We demonstrated that a fully-connected feed-forward neural network could predict hourly bike rentals with the highest accuracy and lowest MAPE. Results showed that using a feed-forward neural network yields a 73.2% $r^2$ and 52.7% MAPE. The neural network was followed by the random forest regressor which could predict with 67.9% $r^2$ and 75.9% MAPE. And finally, GBM with 67.2% $r^2$ and 68.5% MAPE.
            <br><br>
            Further research can include using recurrent neural networks and ARIMA, which are specifically made for sequential data.
            </p>

            <p class="rtgd-section-title">References</p>
            <p class="rtgd-section-references">
            [1] Fanaee-T, Hadi, and Gama, Joao, 'Event labeling combining ensemble detectors and background knowledge', Progress in Artificial Intelligence (2013): pp. 1-15, Springer Berlin Heidelberg, https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset
            </p>


  </section>

</body>

</html>